% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/twostageSL.R
\name{twostageSL}
\alias{twostageSL}
\title{Two-stage Super Learner Prediction}
\usage{
twostageSL(
  Y,
  X,
  newX = NULL,
  library.2stage,
  library.1stage,
  twostage,
  family.1,
  family.2,
  family.single,
  method = "method.CC_LS",
  id = NULL,
  verbose = FALSE,
  control = list(),
  cvControl = list(),
  obsWeights = NULL,
  env = parent.frame()
)
}
\arguments{
\item{Y}{The outcome in the training data set. Must be a numeric vector.}

\item{X}{The predictor variables in the training data set, usually a data.frame.}

\item{newX}{The predictor variables in the validation data set. The structure should match X. If missing, uses X for newX.}

\item{library.2stage}{Candidate prediction algorithms in two-stage super learner. A list containing prediction algorithms at stage 1 and stage 2, the prediction algorithms are either a character vector or a list containing character vectors. See details below for examples on the structure. A list of functions included in the SuperLearner package can be found with \code{listWrappers()}.}

\item{library.1stage}{Candidate prediction algorithms in standard super learner. Either a character vector of prediction algorithms or a list containing character vectors. See details below for examples on the structure. A list of functions included in the SuperLearner package can be found with \code{listWrappers()}.}

\item{twostage}{logical; TRUE for implementing two-stage super learner; FALSE for implementing standatd super learner}

\item{family.1}{Error distribution of the stage 1 outcome for two-stage super learner. Currently only allows \code{binomial} to describe the error distribution. Link function information will be ignored and should be contained in the method argument below.}

\item{family.2}{Error distribution of the stage 2 outcome for two-stage super learner. Currently only allows \code{gaussian} to describe the error distribution. Link function information will be ignored and should be contained in the method argument below.}

\item{family.single}{Error distribution of the outcome for standard super learner. Currently only allows \code{gaussian} to describe the error distribution. Link function information will be ignored and should be contained in the method argument below.}

\item{method}{Details on estimating the coefficients for the two-stage super learner and the model to combine the individual algorithms in the library. Currently, the built in option is only "method.CC_LS". CC_LS uses Goldfarb and Idnani's quadratic programming algorithm to calculate the best convex combination of weights to minimize the squared error loss.}

\item{id}{Optional cluster identification variable. For the cross-validation splits, \code{id} forces observations in the same cluster to be in the same validation fold. \code{id} is passed to the prediction and screening algorithms in library.2stage and library.1stage, but be sure to check the individual wrappers as many of them ignore the information.}

\item{verbose}{logical; TRUE for printing progress during the computation (helpful for debugging).}

\item{control}{A list of parameters to control the estimation process. Parameters include \code{saveFitLibrary} and \code{trimLogit}. See \code{\link{SuperLearner.control}} for details.}

\item{cvControl}{A list of parameters to control the cross-validation process. Parameters include \code{V}, \code{stratifyCV}, \code{shuffle} and \code{validRows}. See SuperLearner.CV.control for details.}

\item{obsWeights}{Optional observation weights variable. As with \code{id} above, \code{obsWeights} is passed to the prediction and screening algorithms, but many of the built in wrappers ignore (or can't use) the information. If you are using observation weights, make sure the library you specify uses the information.}

\item{env}{Environment containing the learner functions. Defaults to the calling environment.}
}
\value{
An object with S3 class \code{twostageSL} containing:
\item{call}{The matched call.}
\item{libraryNames}{A character vector with the names of the algorithms in the library. The format is 'predictionAlgorithm_screeningAlgorithm' with '_All' used to denote the prediction algorithm run on all variables in X.}
\item{library.Num}{Number of prediction algorithms in \code{library.2stage} and \code{library.1stage}.}
\item{orig.library}{Returns the prediction algorithms and screening algorithms in each stage of \code{library.2stage} and \code{library.1stage} seperately.}
\item{SL.library}{Returns the prediction algorithms and screening algorithms in \code{library.2stage} and \code{library.1stage}.}
\item{SL.predict}{The predicted values from the two-stage super learner for the rows in \code{newX}.}
\item{coef}{Coefficients for the two-stage super learner.}
\item{library.predict}{A matrix with the predicted values from each algorithm in \code{library.2stage} and \code{library.1stage} for the rows in \code{newX}.}
\item{Z}{The Z matrix (the cross-validated predicted values for each algorithm in \code{library.2stage} and \code{library.1stage}).}
\item{cvRisk}{A numeric vector with the V-fold cross-validated risk estimate for each algorithm in \code{library.2stage} and \code{library.1stage}. Note that this does not contain the CV risk estimate for the two-stage super learner, only the individual algorithms in the library.}
\item{family}{Returns the \code{family.1}, \code{family.2} and \code{family.single} value from above}
\item{fitLibrary}{A list with the fitted objects for each algorithm in \code{library.2stage} and \code{library.1stage} on the full training data set.}
\item{cvfitLibrary}{A list with fitted objects for each algorithm in \code{library.2stage} and \code{library.1stage} on each of \code{v} different training data sets.}
\item{varNames}{A character vector with the names of the variables in \code{X}.}
\item{validRows}{A list containing the row numbers for the V-fold cross-validation step.}
\item{number0}{A dataframe indicating the number of zeros in each of the \code{v} fold.}
\item{method}{A list with the method functions.}
\item{whichScreen}{A logical matrix indicating which variables passed each screening algorithm.}
\item{control}{The \code{control} list.}
\item{cvControl}{The \code{cvControl} list.}
\item{errorsInCVLibrary}{A logical vector indicating if any algorithms experienced an error within the CV step.}
\item{errorsInLibrary}{A logical vector indicating if any algorithms experienced an error on the full data.}
\item{env}{Environment passed into function which will be searched to find the learner functions. Defaults to the calling environment.}
\item{times}{A list that contains the execution time of the twostageSL, plus separate times for model fitting and prediction.}
}
\description{
A Prediction Function for the Two-stage Super Learner. The \code{twostageSL} function takes a training set pair (X,Y) and returns the predicted values based on a validation set.
}
\details{
\code{twostageSL} fits the two-stage super learner prediction algorithm. The weights for each algorithm in \code{library.2stage} and \code{library.1stage} is estimated, along with the fit of each algorithm.

The prescreen algorithms. These algorithms first rank the variables in \code{X} based on either a univariate regression p-value or the \code{randomForest} variable importance. A subset of the variables in \code{X} is selected based on a pre-defined cut-off. With this subset of the X variables, the algorithms in \code{library.2stage} and \code{library.1stage} are then fit.

The twostageSL package contains a few prediction and screening algorithm wrappers. The full list of wrappers can be viewed with \code{listWrappers()}. The design of the twostageSL package is such that the user can easily add their own wrappers.
}
\examples{
## simulate data
set.seed(123)

## training set
n <- 10000
p <- 5
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
colnames(X) <- paste("X", 1:p, sep="")
X <- data.frame(X)
Y <- rep(NA,n)
## probability of outcome being zero
prob <- plogis(1 + X[,1] + X[,2] + X[,1]*X[,2])
g <- rbinom(n,1,prob)
## assign zero outcome
ind <- g==0
Y[ind] <- 0
## assign non-zero outcome
ind <- g==1
Y[ind] <- 10 + X[ind, 1] + sqrt(abs(X[ind, 2] * X[ind, 3])) + X[ind, 2] - X[ind, 3] + rnorm(sum(ind))

## test set
m <- 1000
newX <- matrix(rnorm(m*p), nrow = m, ncol = p)
colnames(newX) <- paste("X", 1:p, sep="")
newX <- data.frame(newX)
newY <- rep(NA,m)
## probability of outcome being zero
newprob <- plogis(1 + newX[,1] + newX[,2] + newX[,1]*newX[,2])
newg <- rbinom(n,1,newprob)
## assign zero outcome
newind <- newg==0
newY[newind] <- 0
## assign non-zero outcome
newind <- g==1
newY[newind] <- 10 + newX[newind, 1] + sqrt(abs(newX[newind, 2] * newX[newind, 3])) + newX[newind, 2] - X[newind, 3] + rnorm(sum(newind))

## generate the Library
twostage.library <- list(stage1=c("SL.glm","SL.mean","SL.earth"),
                        stage2=c("SL.glm","SL.mean","SL.earth"))
onestage.library <- c("SL.glm","SL.mean","SL.earth")

## run the twostage super learner
two <- twostageSL(Y=Y,
                 X=X,
                 newX = newX,
                 library.2stage <- twostage.library,
                 library.1stage <- onestage.library,
                 twostage = TRUE,
                 family.1=binomial,
                 family.2=gaussian,
                 family.single=gaussian,
                 cvControl = list(V = 5))
two
## run the standard super learner
one <- twostageSL(Y=Y,
                 X=X,
                 newX = newX,
                 library.2stage <- twostage.library,
                 library.1stage <- onestage.library,
                 twostage = FALSE,
                 family.1=binomial,
                 family.2=gaussian,
                 family.single=gaussian,
                 cvControl = list(V = 5))
one

## library with screening
twostage.library <- list(stage1=list(c("SL.glm","screen.glmnet"),
                                    c("SL.earth","screen.corP"),
                                    c("SL.mean","All")),
                        stage2=list(c("SL.glm","screen.glmnet"),
                                    c("SL.earth","screen.corP"),
                                    c("SL.mean","All")))
onestage.library <- list(c("SL.glm","screen.glmnet"),
                        c("SL.earth","screen.corP"),
                        c("SL.mean","All"))

## run the twostage super learner
two <- twostageSL(Y=Y,
                 X=X,
                 newX = newX,
                 library.2stage <- twostage.library,
                 library.1stage <- onestage.library,
                 twostage = TRUE,
                 family.1=binomial,
                 family.2=gaussian,
                 family.single=gaussian,
                 cvControl = list(V = 5))
two
## run the standard super learner
one <- twostageSL(Y=Y,
                 X=X,
                 newX = newX,
                 library.2stage <- twostage.library,
                 library.1stage <- onestage.library,
                 twostage = FALSE,
                 family.1=binomial,
                 family.2=gaussian,
                 family.single=gaussian,
                 cvControl = list(V = 5))
one


}
\references{
van der Laan, M. J., Polley, E. C. and Hubbard, A. E. (2008) Super Learner, Statistical Applications of Genetics and Molecular Biology, 6, article 25.
}
\seealso{
\link{SuperLearner}.
}
\author{
Ziyue Wu
}
